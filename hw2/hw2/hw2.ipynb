{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "adb8d87a-0557-40d4-96cf-e134d605399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def top5(myDict):\n",
    "    outputList = []\n",
    "    subsetList = [] #holds all keys that are of same value\n",
    "    \n",
    "    for key in myDict: #get first key value\n",
    "        currentVal = myDict[key]\n",
    "        break\n",
    "        \n",
    "    for each_key in myDict:\n",
    "        if myDict[each_key] == currentVal: #only want to append keys of same val\n",
    "            subsetList.append(each_key)\n",
    "            \n",
    "        else: #once the value differs\n",
    "            subsetList.sort() #sorted keys for current value alphabetically \n",
    "            for key in subsetList: \n",
    "                tup = (key, myDict[key])\n",
    "                outputList.append(tup)\n",
    "                if len(outputList) == 5: return outputList\n",
    "            #if we added all keys for this value, move to next value\n",
    "            subsetList = []\n",
    "            subsetList.append(each_key)\n",
    "            currentVal = myDict[each_key]\n",
    "\n",
    "# Part 1: Preprocessing; works completely\n",
    "stopwords = open(\"stopwords.txt\", \"r\") # Creates a stopword list that we can reference\n",
    "stopWordsList = stopwords.read().splitlines()\n",
    "\n",
    "textfile = open(\"tfidf_docs.txt\", \"r\")\n",
    "documentNames = textfile.read().splitlines()\n",
    "for document in documentNames:\n",
    "    preprocDocString = \"preproc_\" + document.strip()\n",
    "    with open(preprocDocString, \"w\") as preprocDoc: # Create a new preprocx.txt file for each document in tfidf_docs.txt\n",
    "        for everyLine in open(document.strip(), \"r\"):\n",
    "            words = everyLine.split() # Creates a word list that we can go through\n",
    "            for eachWord in words:\n",
    "                cleanWord = re.sub('[\\W]+', '', eachWord) # Remove all characters that are not words or whitespaces\n",
    "                cleanWord = cleanWord.strip() # Remove extra whitespaces\n",
    "                cleanWord = cleanWord.lower() # Convert all the words to lowercase \n",
    "                if cleanWord in stopWordsList:\n",
    "                    continue # If the word we are looking at is in the stop words list, we go to the next word\n",
    "                elif cleanWord.startswith(\"https\"):\n",
    "                    continue # If the word , we go onto the next word\n",
    "                elif cleanWord.endswith(\"ing\"):\n",
    "                    cleanWord  = cleanWord[:-3] # If the word ends with \"ing\", we remove the last three characters, place it into preprocDoc, and go on to the next word\n",
    "                    preprocDoc.write(cleanWord + \" \")\n",
    "                elif cleanWord.endswith(\"ly\"):\n",
    "                    cleanWord  = cleanWord[:-2] # If the word ends with \"ly\", we remove the last two characters, place it into preprocDoc and go on to the next word\n",
    "                    preprocDoc.write(cleanWord + \" \")\n",
    "                elif cleanWord.endswith(\"ment\"):\n",
    "                    cleanWord  = cleanWord[:-4] # If the word ends with \"ment\", we remove the last 4 characters, place it into preprocDoc and go on to the next word\n",
    "                    preprocDoc.write(cleanWord + \" \")\n",
    "                else:\n",
    "                    preprocDoc.write(cleanWord + \" \") # Adds the word into the preproc doc with a single space if it passes all the previous if statements\n",
    "\n",
    "# Part 2: Computing TF-IDF Scores \n",
    "numberOfDocumentsWordIsIn = {}\n",
    "for document in documentNames: # This loop is to count the number of documents the word is in\n",
    "    preprocDocString = \"preproc_\" + document.strip()\n",
    "    preprocDoc = open(preprocDocString, \"r\") # Reads each preprocx.txt file for each document in tfidf_docs.txt\n",
    "    preprocDocWords = preprocDoc.read().split()\n",
    "    wordFrequency = {} # Creates a dictionary of the word frequency for each document\n",
    "    for word in preprocDocWords:\n",
    "        if word in wordFrequency: # If the word is already in the dictionary, increment the count, if not set the count to 1\n",
    "            wordFrequency[word] += 1\n",
    "        else:\n",
    "            wordFrequency[word] = 1\n",
    "    # Compute how many documents the word is in\n",
    "    for word in wordFrequency: # We iterate through word frequency for each document to add to the numberOfDocumentsWordIsIn dictionary\n",
    "        if word in numberOfDocumentsWordIsIn:\n",
    "            numberOfDocumentsWordIsIn[word] += 1\n",
    "        else:\n",
    "            numberOfDocumentsWordIsIn[word] = 1\n",
    "\n",
    "for document in documentNames: # This loop is to find the term frequency and IDF of each word\n",
    "    termFrequency = {} # Sets TF and IDF to emtpy for the new document\n",
    "    inverseDocumentFrequency = {}\n",
    "    preprocDocString = \"preproc_\" + document.strip()\n",
    "    preprocDoc = open(preprocDocString, \"r\") # Reads each preprocx.txt file for each document in tfidf_docs.txt\n",
    "    preprocDocWords = preprocDoc.read().split()\n",
    "    wordFrequency = {} # Creates a dictionary of the word frequency for each document\n",
    "    for word in preprocDocWords:\n",
    "        if word in wordFrequency: # If the word is already in the dictionary, increment the count, if not set the count to 1\n",
    "            wordFrequency[word] += 1\n",
    "        else:\n",
    "            wordFrequency[word] = 1\n",
    "    #Computing Term Frequency\n",
    "    for word in wordFrequency:\n",
    "        if word in termFrequency: # If the word is already in the term frequency dictionary, we go on to the next word, if not we comput the term frequency of the word and put it into the term frequency dictionary\n",
    "            continue\n",
    "        else:\n",
    "            termFrequency[word] = (wordFrequency.get(word) / len(preprocDocWords))\n",
    "    # Computing IDF\n",
    "    for word in termFrequency:\n",
    "        if numberOfDocumentsWordIsIn.get(word) == 0: # If the word is found in no document, the IDF value for that word is 0\n",
    "            inverseDocumentFrequency[word] = 0\n",
    "        else:\n",
    "            inverseDocumentFrequency[word] = (math.log(len(documentNames) / numberOfDocumentsWordIsIn.get(word))) + 1\n",
    "    # Computing TF*IDF, we now have a dictionary of the TF-IDF score for every word in the document we are looking at\n",
    "    tfidfScoreForDoc = {} # Empties the tfidfScoreForDoc dictionary for the new document\n",
    "    for word in termFrequency:\n",
    "        tfidfScoreForDoc[word] = round((termFrequency.get(word) * inverseDocumentFrequency.get(word)), 2)        \n",
    "    # Sorting the TF-IDF Score dictionary, the tfidfScoreDict is now sorted\n",
    "    tfidfScoreList = sorted(tfidfScoreForDoc.items(), key=lambda x:x[1], reverse = True)\n",
    "    tfidfScoreDict = dict(tfidfScoreList)\n",
    "    tfidfDocString = \"tfidf_\" + document.strip()\n",
    "    countOfWord = 0\n",
    "    finalList = []\n",
    "    usedKeysDictionary = {}\n",
    "    first_pair = list(tfidfScoreDict.items())[0]\n",
    "    wordToPrint = str(first_pair[0]) # Gets the word from the first pair\n",
    "    valueToPrint = float(first_pair[1]) # Gets the tfidf score from the first pair\n",
    "    with open(tfidfDocString, \"w\") as tfidfDoc:\n",
    "        print(top5(tfidfScoreDict), file = tfidfDoc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7b200d1d-1a96-4915-bfe1-da29c2e6c52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ipsum': 0.06, 'latin': 0.06, 'lorem': 0.06, '45': 0.04, 'bc': 0.04}"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usedKeysDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "69ed4e7d-8991-49f7-b41c-9d7e2828d2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ipsum', 0.06), ('latin', 0.06), ('lorem', 0.06), ('45', 0.04), ('bc', 0.04)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "69025746-e9cd-44a9-810a-3312c5f4fe49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordToPrint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "bdd91568-b784-4d70-8d01-d6e31e786098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valueToPrint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c1e8db7c-54f4-44ac-89b4-3783fecdf76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"1500s\" < \"anystring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a788bc56-7145-43a7-bc04-466894696c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(\"anystring\") < str(\"anystring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1ed0e9ad-904b-4f49-849c-ef2238fc9a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lorem': 0.07,\n",
       " 'ipsum': 0.07,\n",
       " 'dummy': 0.06,\n",
       " 'typesett': 0.06,\n",
       " 'type': 0.06,\n",
       " 'text': 0.03,\n",
       " 'print': 0.03,\n",
       " 'industry': 0.03,\n",
       " 'industrys': 0.03,\n",
       " 'standard': 0.03,\n",
       " 'ever': 0.03,\n",
       " 'since': 0.03,\n",
       " '1500s': 0.03,\n",
       " 'unknown': 0.03,\n",
       " 'printer': 0.03,\n",
       " 'took': 0.03,\n",
       " 'galley': 0.03,\n",
       " 'scrambled': 0.03,\n",
       " 'make': 0.03,\n",
       " 'specimen': 0.03,\n",
       " 'survived': 0.03,\n",
       " 'five': 0.03,\n",
       " 'centuries': 0.03,\n",
       " 'also': 0.03,\n",
       " 'leap': 0.03,\n",
       " 'electronic': 0.03,\n",
       " 'remain': 0.03,\n",
       " 'essential': 0.03,\n",
       " 'unchanged': 0.03,\n",
       " 'popularised': 0.03,\n",
       " '1960s': 0.03,\n",
       " 'release': 0.03,\n",
       " 'letraset': 0.03,\n",
       " 'sheets': 0.03,\n",
       " 'contain': 0.03,\n",
       " 'passages': 0.03,\n",
       " 'recent': 0.03,\n",
       " 'desktop': 0.03,\n",
       " 'publish': 0.03,\n",
       " 'software': 0.03,\n",
       " 'like': 0.03,\n",
       " 'aldus': 0.03,\n",
       " 'pagemaker': 0.03,\n",
       " 'includ': 0.03,\n",
       " 'versions': 0.03,\n",
       " 'information': 0.03,\n",
       " 'visit': 0.03,\n",
       " '_tiger92claw165': 0.03,\n",
       " 'simp': 0.02,\n",
       " 'book': 0.02,\n",
       " 'edited': 0.02}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfScoreDict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
